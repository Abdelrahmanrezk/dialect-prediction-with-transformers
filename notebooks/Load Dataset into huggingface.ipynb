{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e08fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install emojis\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f666db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659d813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-08 05:41:31.613450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 05:41:31.947437: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:41:31.947474: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 05:41:32.767417: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:41:32.767556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:41:32.767567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Main libraries\n",
    "from data_shuffling_split import *\n",
    "from preprocess_text import *\n",
    "from datasets import load_dataset, list_datasets, Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b853aa",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "The huggingface datasets library is used either to use available datasets on the hub or either to use your own dataset, either this dataset on your machine or on remote access.\n",
    "\n",
    "### Load from avaliable data\n",
    "\n",
    "First of all, we have listed out how many datasets are available in huggingface.\n",
    "\n",
    "One of the datasets we have used before, is the *emotion dataset* that was mentioned by details in second chapter of book *nlp with transformers*, and we have apply on this dataset in this repo:\n",
    "https://github.com/Abdelrahmanrezk/nlp_with_transformers/blob/main/chapter_2/chapter%202%20Text%20Classification%20with%20transformers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75c2dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23908\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus']\n"
     ]
    }
   ],
   "source": [
    "avaliable_dataset = list_datasets()\n",
    "print(len(avaliable_dataset))\n",
    "print(avaliable_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e932638",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "\n",
    "Now to use any of the hugging face datasets we have to load this dataset and it will be directly cashed on your machine for the next time you will use it easily without downloading it again.\n",
    "\n",
    "**what if the dataset is not on the hugging face Hub ??**\n",
    "In this case, you will also use the load_dataset function with its available scripts that provide you with a way to load your own dataset either from your machine or from an online remote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cf52a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/abdelrahman/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 420.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions_data = load_dataset('emotion')\n",
    "emtions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19427e6a",
   "metadata": {},
   "source": [
    "## DatasetDict\n",
    "It looks like a python dictionary each part of the data is split into keys which defines it as either a train or validation or test, and the value is the split part of the data itself, and we can deal with that as it's a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d766fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emtions_data['train']\n",
    "print(type(train_ds))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d519e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Each of the splited parts in the datasetdict itself is a dataset object which is one of the core data structures in the hugging face dataset library, and we can work with as ordinary Python array or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c089dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "{'text': 'i didnt feel humiliated', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028167b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n",
      "==================================================\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994aa11",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We can see that the output of the train_ds when we have printed out the first index looks like a python dictionary with keys corresponding to the names of the columns in the training dataset, and the values of these keys is the text and labels of that text.\n",
    "\n",
    "Actually the hugging face dataset is built over the *apache spark* which is more memory efficient than native python.\n",
    "\n",
    "Also, we can see that the features of the training dataset printing out the data type of each column in the dataset, and we can see that the label column is a Class Label object which defines the corresponding classes to each label we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7942dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n",
      "==================================================\n",
      "[0, 0, 3, 2, 3]\n",
      "==================================================\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds['text'][:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds['label'][:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds.features['label']._str2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c0900",
   "metadata": {},
   "source": [
    "## Other Dataset\n",
    "\n",
    "We will use another dataset from this paper :\n",
    "https://arxiv.org/pdf/2005.06557.pdf\n",
    "\n",
    "We already got this dataset from another project:\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Arabic-Dialect-Identification\n",
    "\n",
    "So now we are going to know how to load this data set and push it into huggingface hub, but you need to have an account on huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57d71b",
   "metadata": {},
   "source": [
    "## Use our datasets\n",
    "First of all we have splited out the dataset into:\n",
    "- train\n",
    "- test\n",
    "- validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e6ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>839595752783560704</td>\n",
       "      <td>OM</td>\n",
       "      <td>@fahad_laporta يا خي ما حد مبرد قلوبنا مثل قرا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631897776033427456</td>\n",
       "      <td>LB</td>\n",
       "      <td>@Kalamennas @Marcel_Ghanem الشعب ماخمل الشعب ق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>903332474683580416</td>\n",
       "      <td>KW</td>\n",
       "      <td>@AyoubKw يقول قرار تخصيصك تستلمه بعد العيد بس ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768492742733340672</td>\n",
       "      <td>LB</td>\n",
       "      <td>#ما_رح_انسى الحلو ما بينتسى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023998295033741184</td>\n",
       "      <td>IQ</td>\n",
       "      <td>@PrideOfMUFC هذي اخلاقك العالية تخليك تشتم \\nب...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id dialect  \\\n",
       "0   839595752783560704      OM   \n",
       "1   631897776033427456      LB   \n",
       "2   903332474683580416      KW   \n",
       "3   768492742733340672      LB   \n",
       "4  1023998295033741184      IQ   \n",
       "\n",
       "                                                text  \n",
       "0  @fahad_laporta يا خي ما حد مبرد قلوبنا مثل قرا...  \n",
       "1  @Kalamennas @Marcel_Ghanem الشعب ماخمل الشعب ق...  \n",
       "2  @AyoubKw يقول قرار تخصيصك تستلمه بعد العيد بس ...  \n",
       "3                        #ما_رح_انسى الحلو ما بينتسى  \n",
       "4  @PrideOfMUFC هذي اخلاقك العالية تخليك تشتم \\nب...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"../dataset/train/strat_train_set.csv\")\n",
    "strat_dev_set   = read_file(\"../dataset/validation/strat_dev_set.csv\")\n",
    "strat_test_set  = read_file(\"../dataset/test/strat_test_set.csv\")\n",
    "\n",
    "# strat_train_set.columns = ['id', 'dialect', 'dialect_l_encoded', 'text']\n",
    "# strat_dev_set.columns   = ['id', 'dialect', 'dialect_l_encoded', 'text']\n",
    "# strat_test_set.columns  = ['id', 'dialect', 'dialect_l_encoded', 'text']\n",
    "\n",
    "print(type(strat_train_set))\n",
    "\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset Apache arrow\n",
    "ds_strat_train_set = Dataset.from_pandas(strat_train_set)\n",
    "ds_strat_dev_set   = Dataset.from_pandas(strat_dev_set)\n",
    "ds_strat_test_set  = Dataset.from_pandas(strat_test_set)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(type(ds_strat_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0028d2",
   "metadata": {},
   "source": [
    "# Convert dialect string to class label\n",
    "\n",
    "In this case we can easily convert to the correspond dialect when we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(ds_strat_train_set['label']))\n",
    "print(labels)\n",
    "print(\"=\"*50)\n",
    "print(len(labels))\n",
    "print(\"=\"*50)\n",
    "ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "print(ClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52654e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how the labels are string\n",
    "print(ds_strat_train_set.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Labels to IDs\n",
    "def map_dialect_str2int(data):\n",
    "    data['label'] = ClassLabels.str2int(data['label'])\n",
    "    return data\n",
    "\n",
    "ds_strat_train_set = ds_strat_train_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_train_set = ds_strat_train_set.cast_column('label', ClassLabels)\n",
    "\n",
    "\n",
    "\n",
    "ds_strat_dev_set = ds_strat_dev_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_dev_set = ds_strat_dev_set.cast_column('label', ClassLabels)\n",
    "\n",
    "\n",
    "ds_strat_test_set = ds_strat_test_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_test_set = ds_strat_test_set.cast_column('label', ClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc18d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how the labels are now ClassLabels\n",
    "print(ds_strat_train_set.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d8b8a",
   "metadata": {},
   "source": [
    "# Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Check our conversation ====================\")\n",
    "print(list(strat_train_set['label']) == ClassLabels.int2str(ds_strat_train_set['label']))\n",
    "print(list(strat_dev_set['label'])   == ClassLabels.int2str(ds_strat_dev_set['label']))\n",
    "print(list(strat_test_set['label']) == ClassLabels.int2str(ds_strat_test_set['label']))\n",
    "\n",
    "print(list(strat_train_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_train_set['label'][:5]))\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(list(strat_dev_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_dev_set['label'][:5]))\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(list(strat_test_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_test_set['label'][:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets = DatasetDict()\n",
    "\n",
    "dialect_datasets['train']      = ds_strat_train_set\n",
    "dialect_datasets['validation'] = ds_strat_dev_set\n",
    "dialect_datasets['test']       = ds_strat_test_set\n",
    "\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db215ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialect_datasets['train'].features)\n",
    "print(\"=\"*50)\n",
    "print(dialect_datasets['validation'].features)\n",
    "print(\"=\"*50)\n",
    "print(dialect_datasets['test'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977717",
   "metadata": {},
   "source": [
    "# Push the data into the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets.push_to_hub('Abdelrahman-Rezk/Arabic_Dialect_Identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets = load_dataset('Abdelrahman-Rezk/Arabic_Dialect_Identification')\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
