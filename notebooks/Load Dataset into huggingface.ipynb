{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e08fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install emojis\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f666db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659d813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "from data_shuffling_split import *\n",
    "from preprocess_text import *\n",
    "from datasets import load_dataset, list_datasets, Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b853aa",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "The huggingface datasets library is used either to use available datasets on the hub or either to use your own dataset, either this dataset on your machine or on remote access.\n",
    "\n",
    "### Load from avaliable data\n",
    "\n",
    "First of all, we have listed out how many datasets are available in huggingface.\n",
    "\n",
    "One of the datasets we have used before, is the *emotion dataset* that was mentioned by details in second chapter of book *nlp with transformers*, and we have apply on this dataset in this repo:\n",
    "https://github.com/Abdelrahmanrezk/nlp_with_transformers/blob/main/chapter_2/chapter%202%20Text%20Classification%20with%20transformers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75c2dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23903\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus']\n"
     ]
    }
   ],
   "source": [
    "avaliable_dataset = list_datasets()\n",
    "print(len(avaliable_dataset))\n",
    "print(avaliable_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e932638",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "\n",
    "Now to use any of the hugging face datasets we have to load this dataset and it will be directly cashed on your machine for the next time you will use it easily without downloading it again.\n",
    "\n",
    "**what if the dataset is not on the hugging face Hub ??**\n",
    "In this case, you will also use the load_dataset function with its available scripts that provide you with a way to load your own dataset either from your machine or from an online remote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cf52a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/abdelrahman/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 486.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions_data = load_dataset('emotion')\n",
    "emtions_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19427e6a",
   "metadata": {},
   "source": [
    "## DatasetDict\n",
    "It looks like a python dictionary each part of the data is split into keys which defines it as either a train or validation or test, and the value is the split part of the data itself, and we can deal with that as it's a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d766fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emtions_data['train']\n",
    "print(type(train_ds))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d519e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Each of the splited parts in the datasetdict itself is a dataset object which is one of the core data structures in the hugging face dataset library, and we can work with as ordinary Python array or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c089dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "{'text': 'i didnt feel humiliated', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "028167b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n",
      "==================================================\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994aa11",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We can see that the output of the train_ds when we have printed out the first index looks like a python dictionary with keys corresponding to the names of the columns in the training dataset, and the values of these keys is the text and labels of that text.\n",
    "\n",
    "Actually the hugging face dataset is built over the *apache spark* which is more memory efficient than native python.\n",
    "\n",
    "Also, we can see that the features of the training dataset printing out the data type of each column in the dataset, and we can see that the label column is a Class Label object which defines the corresponding classes to each label we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7942dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n",
      "==================================================\n",
      "[0, 0, 3, 2, 3]\n",
      "==================================================\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds['text'][:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds['label'][:5])\n",
    "print(\"=\"*50)\n",
    "print(train_ds.features['label']._str2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c0900",
   "metadata": {},
   "source": [
    "## Other Dataset\n",
    "\n",
    "We will use another dataset from this paper :\n",
    "https://arxiv.org/pdf/2005.06557.pdf\n",
    "\n",
    "We already got this dataset from another project:\n",
    "\n",
    "https://github.com/Abdelrahmanrezk/Arabic-Dialect-Identification\n",
    "\n",
    "So now we are going to know how to load this data set and push it into huggingface hub, but you need to have an account on huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57d71b",
   "metadata": {},
   "source": [
    "## Use our datasets\n",
    "First of all we have splited out the dataset into:\n",
    "- train\n",
    "- test\n",
    "- validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e6ff8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/dialect_data/train/strat_train_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m strat_train_set \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset/dialect_data/train/strat_train_set.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m strat_dev_set   \u001b[38;5;241m=\u001b[39m read_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dataset/dialect_data/validation/strat_dev_set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m strat_test_set  \u001b[38;5;241m=\u001b[39m read_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dataset/dialect_data/test/strat_test_set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/work/nlp/nlp with transformers/dialect-prediction-with-transformers/notebooks/../assets/data_shuffling_split.py:169\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(file_path):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    The function used to read csv file.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        readed_data : the file we have readed\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     readed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m readed_data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/dialect_data/train/strat_train_set.csv'"
     ]
    }
   ],
   "source": [
    "strat_train_set = read_file(\"../dataset/dialect_data/train/strat_train_set.csv\")\n",
    "strat_dev_set   = read_file(\"../dataset/dialect_data/validation/strat_dev_set.csv\")\n",
    "strat_test_set  = read_file(\"../dataset/dialect_data/test/strat_test_set.csv\")\n",
    "\n",
    "strat_train_set.columns = ['id', 'label', 'text']\n",
    "strat_dev_set.columns   = ['id', 'label', 'text']\n",
    "strat_test_set.columns  = ['id', 'label', 'text']\n",
    "\n",
    "print(type(strat_train_set))\n",
    "\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset Apache arrow\n",
    "ds_strat_train_set = Dataset.from_pandas(strat_train_set)\n",
    "ds_strat_dev_set   = Dataset.from_pandas(strat_dev_set)\n",
    "ds_strat_test_set  = Dataset.from_pandas(strat_test_set)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(type(ds_strat_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0028d2",
   "metadata": {},
   "source": [
    "# Convert dialect string to class label\n",
    "\n",
    "In this case we can easily convert to the correspond dialect when we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(ds_strat_train_set['label']))\n",
    "print(labels)\n",
    "print(\"=\"*50)\n",
    "print(len(labels))\n",
    "print(\"=\"*50)\n",
    "ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "print(ClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52654e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how the labels are string\n",
    "print(ds_strat_train_set.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Labels to IDs\n",
    "def map_dialect_str2int(data):\n",
    "    data['label'] = ClassLabels.str2int(data['label'])\n",
    "    return data\n",
    "\n",
    "ds_strat_train_set = ds_strat_train_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_train_set = ds_strat_train_set.cast_column('label', ClassLabels)\n",
    "\n",
    "\n",
    "\n",
    "ds_strat_dev_set = ds_strat_dev_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_dev_set = ds_strat_dev_set.cast_column('label', ClassLabels)\n",
    "\n",
    "\n",
    "ds_strat_test_set = ds_strat_test_set.map(map_dialect_str2int, batched=True)\n",
    "# Casting label column to ClassLabel Object\n",
    "ds_strat_test_set = ds_strat_test_set.cast_column('label', ClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc18d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how the labels are now ClassLabels\n",
    "print(ds_strat_train_set.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d8b8a",
   "metadata": {},
   "source": [
    "# Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Check our conversation ====================\")\n",
    "print(list(strat_train_set['label']) == ClassLabels.int2str(ds_strat_train_set['label']))\n",
    "print(list(strat_dev_set['label'])   == ClassLabels.int2str(ds_strat_dev_set['label']))\n",
    "print(list(strat_test_set['label']) == ClassLabels.int2str(ds_strat_test_set['label']))\n",
    "\n",
    "print(list(strat_train_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_train_set['label'][:5]))\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(list(strat_dev_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_dev_set['label'][:5]))\n",
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(list(strat_test_set['label'])[:5])\n",
    "print(ClassLabels.int2str(ds_strat_test_set['label'][:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets = DatasetDict()\n",
    "\n",
    "dialect_datasets['train']      = ds_strat_train_set\n",
    "dialect_datasets['validation'] = ds_strat_dev_set\n",
    "dialect_datasets['test']       = ds_strat_test_set\n",
    "\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db215ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialect_datasets['train'].features)\n",
    "print(\"=\"*50)\n",
    "print(dialect_datasets['validation'].features)\n",
    "print(\"=\"*50)\n",
    "print(dialect_datasets['test'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977717",
   "metadata": {},
   "source": [
    "# Push the data into the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets.push_to_hub('Abdelrahman-Rezk/Arabic_Dialect_Identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets = load_dataset('Abdelrahman-Rezk/Arabic_Dialect_Identification')\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
