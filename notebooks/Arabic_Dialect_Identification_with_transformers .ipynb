{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ea61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a74e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703331d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "from data_shuffling_split import *\n",
    "from preprocess_text import *\n",
    "from datasets import load_dataset, list_datasets, Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets = load_dataset('Abdelrahman-Rezk/Arabic_Dialect_Identification', use_auth_token=True)\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ea9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dialect_datasets['train']\n",
    "print(train_data.column_names)\n",
    "print(\"=\"*50)\n",
    "print(train_data[0])\n",
    "print(\"=\"*50)\n",
    "print(train_data.features)\n",
    "print(\"=\"*50)\n",
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"pandas\")\n",
    "df = train_data[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return train_data.features['label'].int2str(row)\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens per tweet'] = df['text'].str.split().apply(len)\n",
    "df.boxplot(\"tokens per tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b76a0",
   "metadata": {},
   "source": [
    "## From Text to Tokens\n",
    "\n",
    "- تحديد انهى موديلز هنستخدمها\n",
    "- تحديد التكوينزر الخاص بكل موديل\n",
    "- التجريب على بعض الأمثلة من الداتا\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-did-madar-twitter5?text=%D8%B9%D8%A7%D9%85%D9%84+%D8%A7%D9%8A%D9%87+%D8%9F\n",
    "\n",
    "Edit tokenizer in our model to be limited to 124 word !?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d2a8d",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb42b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07294a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = train_data['text'][8]\n",
    "some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(some_text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(train_data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch['text'] = \"الهدف من الحياة هو سث[MASK] . \"\n",
    "encoded_text = tokenize(batch)\n",
    "print(encoded_text)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576cb6d",
   "metadata": {},
   "source": [
    "### Model Special Token & ITs ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31394e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=None to get the length inputs based on the large tweet\n",
    "dialect_datasets_encoded = dialect_datasets.map(tokenize, batched=True, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialect_datasets_encoded.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AutoModel class converts the token encodings to embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c10fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
