{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4f8a90-9501-4f2c-af68-42093128d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install sklearn\n",
    "# !pip install emtions\n",
    "# !pip install ipywidgets\n",
    "# !pip install emojis\n",
    "# !pip3 install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3ea61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a74e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/abdelrahman/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703331d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "from data_shuffling_split import *\n",
    "from preprocess_text import *\n",
    "from datasets import load_dataset, list_datasets, Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e7343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Abdelrahman-Rezk--Arabic_Dialect_Identification-256d82f412398eec\n",
      "Reusing dataset parquet (/home/abdelrahman/.cache/huggingface/datasets/Abdelrahman-Rezk___parquet/Abdelrahman-Rezk--Arabic_Dialect_Identification-256d82f412398eec/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e6ceb50cec478e8532f9fbf4f90a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 9164\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 440052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 8981\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_datasets = load_dataset('Abdelrahman-Rezk/Arabic_Dialect_Identification', use_auth_token=True)\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826ea9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'label', 'text']\n",
      "==================================================\n",
      "{'id': 1046024946705735552, 'label': 9, 'text': '@Ahmed_Hamza27 Ø§ØªÙ…Ù†Ù‰ ØµØ±Ø§Ø­Ø© Ùˆ Ø®ØµÙˆØµØ§ #Ø·Ø§Ù„_Ø¹Ù…Ø±Ù‡ Ø¨ÙŠÙ†Ø§ ÙˆØ¨ÙŠÙ†Ù‡ Ù‚ØµØ© Ø¹Ø´Ù‚ Ù‚Ø¯ÙŠÙ…Ø©ðŸ˜ðŸ˜'}\n",
      "==================================================\n",
      "{'id': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=18, names=['OM', 'SD', 'SA', 'KW', 'QA', 'LB', 'JO', 'SY', 'IQ', 'MA', 'EG', 'PL', 'YE', 'BH', 'DZ', 'AE', 'TN', 'LY'], id=None), 'text': Value(dtype='string', id=None)}\n",
      "==================================================\n",
      "{'id': [1046024946705735552, 1140189387508134016, 1051416181855440768], 'label': [9, 17, 0], 'text': ['@Ahmed_Hamza27 Ø§ØªÙ…Ù†Ù‰ ØµØ±Ø§Ø­Ø© Ùˆ Ø®ØµÙˆØµØ§ #Ø·Ø§Ù„_Ø¹Ù…Ø±Ù‡ Ø¨ÙŠÙ†Ø§ ÙˆØ¨ÙŠÙ†Ù‡ Ù‚ØµØ© Ø¹Ø´Ù‚ Ù‚Ø¯ÙŠÙ…Ø©ðŸ˜ðŸ˜', '@rJiM4CnIFTgml9g @zamnissi Ø·Ø¨Ø¹Ø§ Ø¹Ø±ÙØªÙ‡ Ù…Ù† Ø§Ù„Ù„ÙŠ Ø¨Ø´Ø¹Ø§Ø± Ø§Ù„Ù„ÙŠÚ¤Ø±', '@OmanisFollowers @ooredoo @TRA_OMAN @motc_om Ù‡ÙŠÙ‡ Ù‡ÙŠÙ‡ Ù… ÙŠØªØºÙŠØ± Ø´ÙŠ.. ÙƒÙ„Ù‡Ù… Ø¹ Ø¨Ø¹Ø¶Ù‡Ù… Ù…ØªÙˆÙ„ÙÙŠÙ†ðŸ˜ª.. Ø¨ÙŠØ¬ÙŠÙƒ Ù…Ø¯ÙŠØ± ÙØ±Ø¹ Ù‚Ø±ÙŠØ¨ Ù…Ù†Ùƒ Ø¹Ø´Ø§Ù† ÙŠØªØ³Ø§Ø¹Ø¯ Ù…Ø¹Ùƒ Ø¹Ø´Ø§Ù† Ø¨Ø³ ÙŠØ«Ø¨Øª Ø¬Ø¯Ø§Ø±ØªÙ‡ Ù„ÙˆØ¸ÙŠÙÙŠÙ‡ ÙÙƒØ±Ø³ÙŠÙ‡ ÙˆÙŠØ­Ø§ÙˆÙ„ ÙŠØªØ¹Ø§ÙˆÙ† Ù…Ø¹Ùƒ Ù„ÙƒÙ† ÙØ§Ù„Ù†Ù‡Ø§ÙŠÙ‡ Ø¨ØªØ±Ø¬Ø¹ Ø¹ Ù†ÙØ³ Ø§Ù„Ù…Ø´ÙƒÙ„Ù‡ ÙˆØ§Ù„Ø¯Ø§Ø¦Ø±Ø© ØªØ¯ÙˆØ± Ù„Ø§ Ø¨ÙŠÙ‚Ø¯Ù…ÙˆØ§ ÙˆÙ„Ø§ Ø¨ÙŠØ£Ø®Ø±ÙˆØ§ðŸ˜ªðŸ˜ª.. ÙˆØ­Ø¯ÙŠ ØºØ§Ø³Ù„Ù‡ Ø§ÙŠØ¯ÙŠ Ù…Ù†Ù‡Ù…']}\n"
     ]
    }
   ],
   "source": [
    "train_data = dialect_datasets['train']\n",
    "print(train_data.column_names)\n",
    "print(\"=\"*50)\n",
    "print(train_data[0])\n",
    "print(\"=\"*50)\n",
    "print(train_data.features)\n",
    "print(\"=\"*50)\n",
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"pandas\")\n",
    "df = train_data[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return train_data.features['label'].int2str(row)\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens per tweet'] = df['text'].str.split().apply(len)\n",
    "df.boxplot(\"tokens per tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b76a0",
   "metadata": {},
   "source": [
    "## From Text to Tokens\n",
    "\n",
    "- ØªØ­Ø¯ÙŠØ¯ Ø§Ù†Ù‡Ù‰ Ù…ÙˆØ¯ÙŠÙ„Ø² Ù‡Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§\n",
    "- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø²Ø± Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒÙ„ Ù…ÙˆØ¯ÙŠÙ„\n",
    "- Ø§Ù„ØªØ¬Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¯Ø§ØªØ§\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-did-madar-twitter5?text=%D8%B9%D8%A7%D9%85%D9%84+%D8%A7%D9%8A%D9%87+%D8%9F\n",
    "\n",
    "Edit tokenizer in our model to be limited to 124 word !?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d2a8d",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb42b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07294a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = train_data['text'][8]\n",
    "some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(some_text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(train_data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch['text'] = \"Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø§Ù„Ø­ÙŠØ§Ø© Ù‡Ùˆ Ø³Ø«[MASK] . \"\n",
    "encoded_text = tokenize(batch)\n",
    "print(encoded_text)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576cb6d",
   "metadata": {},
   "source": [
    "### Model Special Token & ITs ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31394e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=None to get the length inputs based on the large tweet\n",
    "dialect_datasets_encoded = dialect_datasets.map(tokenize, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialect_datasets_encoded.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AutoModel class converts the token encodings to embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51765052",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(some_text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ac0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab555ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, n_tokens, hidden_dim]\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    # print(\"=\"*50)\n",
    "    # print(tokenizer.model_input_names)\n",
    "    # print(batch.items())\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_hidden = dialect_datasets_encoded.map(extract_hidden_states, batched=True,  batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2a609-09d2-4beb-a29a-d9b4fb5e822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "# axes = axes.flatten()\n",
    "# cmaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "#                       'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "#                       'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "# labels = dialect_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "# for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "#     df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "#     axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap, gridsize=20, linewidths=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064429e5-57bf-48a0-83df-95cca932ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(train_data['label']))\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5434c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1364fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids # acutall\n",
    "    preds = pred.predictions.argmax(-1) # predicted\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "logging_steps = len(dialect_datasets[\"train\"]) // batch_size\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-Arabic_Dialect_Identification_model_1\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                num_train_epochs=5,\n",
    "                                learning_rate=2e-5,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                weight_decay=0.01,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                disable_tqdm=False,\n",
    "                                logging_steps=logging_steps,\n",
    "                                push_to_hub=True,\n",
    "                                log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                train_dataset=dialect_datasets[\"train\"],\n",
    "                eval_dataset=dialect_datasets[\"validation\"],\n",
    "                tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce984",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(dialect_datasets[\"validation\"])\n",
    "print(preds_output)\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = dialect_datasets[\"validation\"]['label']\n",
    "labels = dialect_datasets[\"train\"].features[\"label\"].names\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
