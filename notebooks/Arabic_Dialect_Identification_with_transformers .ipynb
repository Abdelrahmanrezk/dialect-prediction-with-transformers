{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4f8a90-9501-4f2c-af68-42093128d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install sklearn\n",
    "# !pip install emtions\n",
    "# !pip install ipywidgets\n",
    "# !pip install emojis\n",
    "# !pip3 install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3ea61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a74e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/abdelrahman/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703331d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "from data_shuffling_split import *\n",
    "from preprocess_text import *\n",
    "from datasets import load_dataset, list_datasets, Dataset, DatasetDict, ClassLabel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e7343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Abdelrahman-Rezk--Arabic_Dialect_Identification-256d82f412398eec\n",
      "Reusing dataset parquet (/home/abdelrahman/.cache/huggingface/datasets/Abdelrahman-Rezk___parquet/Abdelrahman-Rezk--Arabic_Dialect_Identification-256d82f412398eec/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e6ceb50cec478e8532f9fbf4f90a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 9164\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 440052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label', 'text'],\n",
       "        num_rows: 8981\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_datasets = load_dataset('Abdelrahman-Rezk/Arabic_Dialect_Identification', use_auth_token=True)\n",
    "dialect_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826ea9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'label', 'text']\n",
      "==================================================\n",
      "{'id': 1046024946705735552, 'label': 9, 'text': '@Ahmed_Hamza27 اتمنى صراحة و خصوصا #طال_عمره بينا وبينه قصة عشق قديمة😍😍'}\n",
      "==================================================\n",
      "{'id': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=18, names=['OM', 'SD', 'SA', 'KW', 'QA', 'LB', 'JO', 'SY', 'IQ', 'MA', 'EG', 'PL', 'YE', 'BH', 'DZ', 'AE', 'TN', 'LY'], id=None), 'text': Value(dtype='string', id=None)}\n",
      "==================================================\n",
      "{'id': [1046024946705735552, 1140189387508134016, 1051416181855440768], 'label': [9, 17, 0], 'text': ['@Ahmed_Hamza27 اتمنى صراحة و خصوصا #طال_عمره بينا وبينه قصة عشق قديمة😍😍', '@rJiM4CnIFTgml9g @zamnissi طبعا عرفته من اللي بشعار الليڤر', '@OmanisFollowers @ooredoo @TRA_OMAN @motc_om هيه هيه م يتغير شي.. كلهم ع بعضهم متولفين😪.. بيجيك مدير فرع قريب منك عشان يتساعد معك عشان بس يثبت جدارته لوظيفيه فكرسيه ويحاول يتعاون معك لكن فالنهايه بترجع ع نفس المشكله والدائرة تدور لا بيقدموا ولا بيأخروا😪😪.. وحدي غاسله ايدي منهم']}\n"
     ]
    }
   ],
   "source": [
    "train_data = dialect_datasets['train']\n",
    "print(train_data.column_names)\n",
    "print(\"=\"*50)\n",
    "print(train_data[0])\n",
    "print(\"=\"*50)\n",
    "print(train_data.features)\n",
    "print(\"=\"*50)\n",
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"pandas\")\n",
    "df = train_data[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return train_data.features['label'].int2str(row)\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens per tweet'] = df['text'].str.split().apply(len)\n",
    "df.boxplot(\"tokens per tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bd220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b76a0",
   "metadata": {},
   "source": [
    "## From Text to Tokens\n",
    "\n",
    "- تحديد انهى موديلز هنستخدمها\n",
    "- تحديد التكوينزر الخاص بكل موديل\n",
    "- التجريب على بعض الأمثلة من الداتا\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa\n",
    "\n",
    "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa-did-madar-twitter5?text=%D8%B9%D8%A7%D9%85%D9%84+%D8%A7%D9%8A%D9%87+%D8%9F\n",
    "\n",
    "Edit tokenizer in our model to be limited to 124 word !?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d2a8d",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb42b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07294a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = train_data['text'][8]\n",
    "some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(some_text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "print(tokenize(train_data[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {}\n",
    "batch['text'] = \"الهدف من الحياة هو سث[MASK] . \"\n",
    "encoded_text = tokenize(batch)\n",
    "print(encoded_text)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576cb6d",
   "metadata": {},
   "source": [
    "### Model Special Token & ITs ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31394e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=None to get the length inputs based on the large tweet\n",
    "dialect_datasets_encoded = dialect_datasets.map(tokenize, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialect_datasets_encoded.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AutoModel class converts the token encodings to embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51765052",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(some_text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ac0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab555ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, n_tokens, hidden_dim]\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    # print(\"=\"*50)\n",
    "    # print(tokenizer.model_input_names)\n",
    "    # print(batch.items())\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_hidden = dialect_datasets_encoded.map(extract_hidden_states, batched=True,  batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_datasets_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2a609-09d2-4beb-a29a-d9b4fb5e822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "# axes = axes.flatten()\n",
    "# cmaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "#                       'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "#                       'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "# labels = dialect_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "# for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "#     df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "#     axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap, gridsize=20, linewidths=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064429e5-57bf-48a0-83df-95cca932ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(train_data['label']))\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5434c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1364fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids # acutall\n",
    "    preds = pred.predictions.argmax(-1) # predicted\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "logging_steps = len(dialect_datasets[\"train\"]) // batch_size\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-Arabic_Dialect_Identification_model_1\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                num_train_epochs=5,\n",
    "                                learning_rate=2e-5,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                weight_decay=0.01,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                disable_tqdm=False,\n",
    "                                logging_steps=logging_steps,\n",
    "                                push_to_hub=True,\n",
    "                                log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                train_dataset=dialect_datasets[\"train\"],\n",
    "                eval_dataset=dialect_datasets[\"validation\"],\n",
    "                tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce984",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(dialect_datasets[\"validation\"])\n",
    "print(preds_output)\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = dialect_datasets[\"validation\"]['label']\n",
    "labels = dialect_datasets[\"train\"].features[\"label\"].names\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
